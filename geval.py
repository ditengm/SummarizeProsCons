from datasets import load_dataset
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import re
import torch
import pandas as pd


from datasets import load_dataset
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import re
import torch
import pandas as pd

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

GEVAL_TEMPLATE_PROMPT = """
You will be given a review and its pros and cons, which are generated by the model.
Your task is to evaluate how the model generated the pros and cons according to the review.
Please make sure that you have read and understood this review carefully.
Please keep this document in mind during the assessment and refer to it if necessary. 
If there is the word 'No' in the pros or cons, it means that the model has not found the pros or cons.
It is very important, there may be a completely different wording in the review than in the pros and cons, do not pay attention to it
Don't be too strict, but give me a fair assessment.
The score can be from 1 to 5.
Score is the quality of how well the generated pros and cons reflect the content and key points of the review provided.
Score 1 - The generated pros and cons are unrelated to the content of the review or poorly reflect its key points.
Score 2 - Some of the generated pros and cons reflect elements mentioned in the review, but many important points are missed or inaccurately represented.
Score 3 - The majority of the generated pros and cons reflect elements mentioned in the review, but there are some inaccuracies or omissions.
Score 4 - The generated pros and cons closely reflect the content of the review, capturing most of the important points accurately.
Score 5 - The generated pros and cons accurately and comprehensively reflect the content of the review, capturing all important points with precision.

Example 1:
review: This was a very informative book. Not a novel but still very engaging. did not know their influence in many areas of things we take for granted.
pros: Informative, Engaging, Eye-opening
cons: No
Score: 5

Example 2:
review = Not nice quality but cute print.
pros = Cute print
cons = Bad quality
Score: 5

Example 3:
Review: This smartphone offers excellent performance and a sleek design. The display is vibrant and sharp, making it perfect for multimedia consumption. The battery life is decent, lasting through a full day of moderate usage. However, the camera quality could be improved, especially in low-light conditions. Overall, it's a solid choice for those seeking a reliable and stylish device.
Pros: Excellent performance, Sleek design, Vibrant display, Decent battery life
Cons: Camera quality in low-light conditions could be improved
Score: 4

Example 4:
review: I bought this as a gift and on sale - they loved how this comes with a tote. The tools feel well made and it has a great variety of items.
pros = Sale, Tote, Well made, Variety
cons = Not durable, Limited functionality, Poor quality
Score: 3

Example 5:
Review: This toaster fails to toast bread properly. The control buttons are too complicated to use, and the instructions don't help to figure it out. There are frequent malfunctions, leading to improper toasting or breakdowns. Additionally, the design is too bulky and takes up a lot of space on the kitchen countertop.
Pros: Sleek design, Easy to use controls, Reliable performance
Cons: Comfortable, Cute
Score: 1

In the answer, write only the number, only 1, 2, 3, 4 or 5 without explanations and unnecessary words.

Steps in evaluating the content of a model generation:
1. Read the review carefully to identify its key points and understand the reviewer's opinion.
2. Evaluate the pros and cons generated by the model to determine how well they reflect the content and key points of the review.
3. Compare the pros and cons to the review, focusing on the accuracy, comprehensiveness, and relevance of each point.
4. Assign a score based on the quality of the generated pros and cons, using the given scale (1-5) to indicate how closely they reflect the review.


Review: {review}
Pros: {pros}
Cons: {cons}
Score (write only 1, 2, 3, 4, or 5):
"""

class GEval:
    """
    Класс GEval предназначен для оценки сгенерированного текста с использованием предобученной языковой модели.
    
    Атрибуты:
        model (AutoModelForCausalLM): Экземпляр предобученной языковой модели для генерации текста.
        tokenizer (AutoTokenizer): Токенизатор, соответствующий модели, для преобразования текста в токены и обратно.
        score_tokens (list[int]): Список идентификаторов токенов, используемых для оценки сгенерированного текста.
        weight_tokens (torch.tensor): Веса, соответствующие токенам оценок, для вычисления итогового балла.
        
    Методы:
        init(self, model_name: str):
            Инициализирует экземпляр класса, загружая модель и токенизатор по указанному имени модели.
            
        find_tokens(self, generated_tokens: list, score_tokens: list) -> list:
            Возвращает индексы токенов из списка generated_tokens, которые присутствуют в списке score_tokens.
            
        predict(self, review: str) -> float:
            Генерирует продолжение текста на основе входного примера review и вычисляет итоговый балл сгенерированного текста.
    
    Пример использования:
        # Инициализация оценщика с предобученной моделью
        g_eval = GEval('gpt2')
        
        # Оценка сгенерированного текста
        score = g_eval.predict("Пример текста для генерации.")
        
        print(f"Итоговый балл: {score}")
    """

    def __init__(self, model_name: str):
        """
        Инициализирует экземпляр класса GEval.
        
        Аргументы:
            model_name (str): Имя предобученной модели, которая будет использоваться для генерации текста.
        """

        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.score_tokens = self.tokenizer.convert_tokens_to_ids(['1', '2', '3', '4', '5'])
        self.weight_tokens = torch.tensor([1., 2., 3., 4., 5.])
        
    def find_tokens(self, generated_tokens: list, score_tokens: list):
        """
        Находит индексы токенов из списка score_tokens в списке generated_tokens.
        
        Аргументы:
            generated_tokens (list): Список идентификаторов сгенерированных токенов.
            score_tokens (list): Список идентификаторов токенов оценок.
        
        Возвращает:
            list: Список индексов токенов оценок в сгенерированном тексте.
        """

        indexies = []
        for i in range(len(generated_tokens)):
            id_token = generated_tokens[i]
            for id_score_token in score_tokens:
                if id_token == id_score_token:
                    indexies.append(i)
        return indexies
        
    def predict(self, review: str, pros: str, cons: str) -> float:
        """
        Генерирует продолжение текста на основе входного примера и вычисляет итоговый балл сгенерированного текста.
        
        Аргументы:
            review (str): Текст, на основе которого будет сгенерировано продолжение.
        
        Возвращает:
            float: Итоговый балл сгенерированного текста.
        """

        prompt = GEVAL_TEMPLATE_PROMPT.format(review=review, pros=pros, cons=cons)
        
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors="pt")
        model_inputs = encodeds.to(self.model.device)
        
        generation = self.model.generate(model_inputs, 
                               max_new_tokens=25, 
                               pad_token_id=self.tokenizer.eos_token_id,
                               output_scores=True,
                               return_dict_in_generate=True,
                               do_sample=False)
        
        generated_ids = generation['sequences']
        all_logits = generation['scores']
        generated_tokens = generated_ids[:, len(model_inputs[0]):][0].tolist()
        decoded = self.tokenizer.batch_decode(generated_ids)
        ids = self.find_tokens(generated_tokens, self.score_tokens)
        try:
            logits_token = all_logits[ids[0] - 1][0]
        except:
            logits_token = all_logits[-1]
            
        log_probs = torch.nn.functional.softmax(logits_token[self.score_tokens], dim=0).detach().cpu()
        score = torch.dot(log_probs, self.weight_tokens).item()
        return score
    
    def get_generation(self, review: str, pros: str, cons: str):
        prompt = GEVAL_TEMPLATE_PROMPT.format(review=review, pros=pros, cons=cons)
        
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors="pt")
        model_inputs = encodeds.to(self.model.device)
        
        generation = self.model.generate(model_inputs, 
                               max_new_tokens=25, 
                               pad_token_id=self.tokenizer.eos_token_id,
                               output_scores=True,
                               return_dict_in_generate=True,
                               do_sample=False)
        all_logits = generation['scores']
        generated_ids = generation['sequences']
        generated_tokens = generated_ids[:, len(model_inputs[0]):][0].tolist()
        decoded = self.tokenizer.batch_decode(generated_tokens)
        return decoded
